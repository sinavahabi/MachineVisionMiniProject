{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4107de54",
   "metadata": {},
   "source": [
    "## In The Name of God\n",
    "\"\"\"\n",
    "\n",
    "    Program: Image Classification (AI)\n",
    "\n",
    "    Author: sina vahabi\n",
    "\n",
    "    Copyright: 2023/04\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "### Project description\n",
    "To specify the differences between aluminum 2000 series alloys and aluminum 7000 series alloys using Convolutional Neural Networks (CNNs), we would need a labeled dataset of images representing both types of alloys. The dataset should contain a sufficient number of images for each class, and the images should capture the characteristic features that differentiate the two series of alloys.\n",
    "\n",
    "Here's an overview of the steps involved in using a CNN for this project:\n",
    "\n",
    "Dataset collection and preparation: We should gather a diverse set of images representing aluminum 2000 series alloys and aluminum 7000 series alloys. Ensuring that the images are properly labeled according to their respective series. Preprocessing the images by resizing them to a consistent resolution and normalizing pixel values to a common scale (which is from 0 to 1 e.g.)\n",
    "\n",
    "Split the dataset: Dividing dataset into training, validation, and testing sets. The training set is used to train the CNN model, the validation set helps in tuning hyperparameters and monitoring performance during training, and the testing set is used to evaluate the final model's performance.\n",
    "\n",
    "Model architecture selection: Choosing a CNN architecture suitable for image classification tasks. These models typically consist of convolutional layers for feature extraction and fully connected layers for classification.\n",
    "\n",
    "Model training: We need to initialize the chosen CNN model with random weights and train it on the training set. During training, the model learns to extract relevant features from the images and make predictions based on those features. Using appropriate optimization algorithms (e.g., stochastic gradient descent or Adam) and loss functions (e.g., categorical cross-entropy) to guide the training process. Adjusting hyperparameters (e.g., learning rate, batch size) based on the performance on the validation set.\n",
    "\n",
    "Model evaluation: Evaluating the trained model's performance using the testing set. So we have to calculate metrics such as accuracy, precision, recall, and F1 score to assess how well the model distinguishes between the aluminum 2000 series and the aluminum 7000 series alloys.\n",
    "\n",
    "Note: '\"'F1 score is an evaluation metric that measures a model's accuracy.\n",
    "It combines the precision and recall scores of a model.\n",
    "\n",
    "The accuracy metric computes how many times a model made a correct prediction across the entire dataset.\n",
    "This metric is calculated as:\n",
    "##### \"F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\"\n",
    "where:\n",
    "\n",
    "    Precision: Correct positive predictions relative to total positive predictions.\n",
    "    \n",
    "    Recall: Correct positive predictions relative to total actual positives.'\"'\n",
    "\n",
    "##### Also note that, F1 score has following interpretation:\n",
    "\n",
    "F1 Score: ------------------------> Interpretation\n",
    "\n",
    "'> 0.9': --------------------------> Very Good \n",
    "\n",
    "'0.8 - 0.9': -----------------------> Good \n",
    "\n",
    "'0.5 - 0.8': -----------------------> Ok \n",
    "\n",
    "'< 0.5': --------------------------> Not Good \n",
    "\n",
    "\n",
    "Fine-tuning and optimization: Depending on the performance of the initial model, we may need to fine-tune the model by adjusting hyperparameters, trying different architectures, or applying techniques like data augmentation to improve the model's generalization and accuracy.\n",
    "\n",
    "Finally, the success of the CNN model depends on the quality and representativeness of the dataset, as well as the careful design of the architecture and training process. It's crucial to have a diverse and balanced dataset that captures the variations between the two series of alloys to achieve accurate and reliable classification results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e37d3921",
   "metadata": {},
   "source": [
    "### We can install libraries using commands bellow:\n",
    "#### pip install tensorflow \n",
    "#### pip install keras\n",
    "#### pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8959c17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing essential libraries after installing theme.\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1558ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the path to our dataset\n",
    "train_dir = 'dataset/train/'\n",
    "test_dir = 'dataset/test/'\n",
    "# Setting the image dimensions and other training parameters.\n",
    "img_width, img_height = 224, 224\n",
    "batch_size = 32\n",
    "# An epoch refers to one cycle through the full training dataset.\n",
    "epochs = 20\n",
    "# 2 classes: Aluminum 2000 series and Aluminum 7000 series\n",
    "num_classes = 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7fb4d93",
   "metadata": {},
   "source": [
    "\"rescale\": Rescaling factor which the value defaults is None. If None or 0, no rescaling is applied, otherwise we multiply the data by the value provided (after applying all other transformations).\n",
    "\n",
    "\"rotation_range\": Integer; degree range for random rotations. In this instance, value is set to 20 degree.\n",
    "\n",
    "\"height_shift_range\": Float, 1-D array-like or integer. In this instance it will take -20% to +20% which means it will shift (horizontally) image randomly between this range.\n",
    "\n",
    "\"width_shift_range\": Float, 1-D array-like or integer. In this instance it will take -20% to +20% which means it will shift (vertically) image randomly between this range.\n",
    "\n",
    "\"horizontal_flip\" : Boolean, randomly flip inputs horizontally.\n",
    "\n",
    "\"validation_split\": A number between 0 and 1 will be pass to \"ImageDataGenerator()\" class instance to split the data into train and validation sets which will be done effectively when we set the \"subset\" attribute to 'training' or 'validation' in codes bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8baab166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and preprocessing.\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84de86b2",
   "metadata": {},
   "source": [
    "\"target_size\": Defines what resolution and size we want our image be; the more the distance is between original image and our CNN model, the more we lose data. So it's better if we keep the distance between our CCN model image size (height & width) and original image size as low as possible.\n",
    "\n",
    "\"batch_size\": It is used to split samples to 32 parts from all of our dataset in order to use less memory and better network performance. For example if we have 3200 images in our dataset, this batch size will divide them to 32 parts where each part contains 100 images.\n",
    "\n",
    "\"class_mode\": The class mode attribute has a few values, and the best one to return 2D one-hot encoded labels, for determining the type of label arrays produced is 'categorical'.\n",
    "\n",
    "\"subset\": Helps us to split train data from validation data, also this will specify training and validation generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4405503e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16589 images belonging to 2 classes.\n",
      "Found 4146 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Generating our train data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Generating our validation data\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "795fad04",
   "metadata": {},
   "source": [
    "\"Sequential()\": The Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor. we can create a \"Sequential()\" model by passing a list of layers to the Sequential constructor. Its layers are accessible via the layers attribute.\n",
    "\n",
    "\"activation='relu'\": Applies the rectified linear unit activation function. With default values, this returns the standard ReLU activation: max(x, 0); the element-wise maximum of 0 and the input tensor. Modifying default parameters allows us to use non-zero thresholds, we can change the max value of the activation, and to use a non-zero multiple of the input for values below the threshold.\n",
    "\n",
    "\"Conv2D()\": The first parameter (32 for the first instance) is the number of filters in the layer. The second parameter (3, 3) is the size of the filter kernel.\n",
    "\n",
    "\"filters\": Integer, the dimensionality of the output space.\n",
    "\n",
    "\"kernel_size\": An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions.\n",
    "\n",
    "\"MaxPooling2D()\": The only parameter used is \"pool_size\" which is either an integer or tuple of 2 integers. (2, 2) will take the max value over a 2x2 pooling window. If only one integer is specified, the same window length will be used for both dimensions.\n",
    "\n",
    "\"Flatten()\": This method flattens the input. Does not affect the batch size by the way. \n",
    "\n",
    "\"Dense()\": Densely-connected NN layer. A dense layer is a layer of neurons in a neural network. Each neuron in the dense layer is connected to every neuron in the previous layer, and each neuron in the dense layer has a weight associated with it.\n",
    "\n",
    "\"units\": The first parameter is \"unit\" attribute which is a positive integer. This positive integer is dimensionality of the output space.\n",
    "\n",
    "\"activation='softmax'\": The softmax activation function is used in neural networks for multiclass classification problems. It transforms the raw outputs of the neural network into a vector of probabilities, essentially a probability distribution over the input classes. The softmax is often used as the activation for the last layer of a classification network because the result could be interpreted as a probability distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56cd3881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the CNN model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d43666df",
   "metadata": {},
   "source": [
    "\"compile()\": This function is used to configure the learning process of a model. It specifies the \"optimizer\", \"loss\" function, and \"metrics\" that the model will use during training.\n",
    "\n",
    "\"optimizer\": This is the optimizer algorithm used to update the weights of the neural network.\n",
    "In this case, it is set to ‘adam’, which is an optimization algorithm that can be used instead of the classical stochastic gradient descent or 'SGD' procedure to update network weights.\n",
    "\n",
    "\"loss\": This is the objective function that the model will try to minimize during training. In this case, it is set to ‘categorical_crossentropy’, which is a loss function used for multi-class classification problems.\n",
    "\n",
    "\"metrics\": This is a list of metrics used to evaluate the performance of your model. In this case, it is set to [‘accuracy’], which means that the accuracy of the model will be evaluated during training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "418f6fc3",
   "metadata": {},
   "source": [
    "IMPORTANT! \n",
    "\n",
    "If we want to have a brief comparison between these 2 optimization algorithms we can say following expressions:\n",
    "##### In summary, while Adam has been widely used across a variety of applications, it is not always the best choice for training deep neural networks on  image classification tasks. 'SGD' is still the most popular optimization algorithm in deep learning and is known to generalize better than Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8f95e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35829648",
   "metadata": {},
   "source": [
    "\"model.fit()\": This is a method used to train a machine learning model. It is used to fit the training data into the model and adjust its weights so that it can make accurate predictions on new data. \"model.fit()\" is typically used in conjunction with other methods such as \"model.predict()\" and \"model.evaluate()\" to build and evaluate machine learning models.\n",
    "\n",
    "\"train_generator\": The generator for the training data.\n",
    "\n",
    "\"steps_per_epoch\": The number of steps (batches) per epoch.\n",
    "\n",
    "\"epochs\": The number of epochs to train the model.\n",
    "\n",
    "\"validation_data\": The generator for the validation data.\n",
    "\n",
    "\"validation_steps\": The number of steps (batches) to validate the model on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1590f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "285/518 [===============>..............] - ETA: 6:17 - loss: 0.7293 - accuracy: 0.5617"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\TiffImagePlugin.py:864: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "518/518 [==============================] - 894s 2s/step - loss: 0.7020 - accuracy: 0.5837 - val_loss: 0.6538 - val_accuracy: 0.6129\n",
      "Epoch 2/20\n",
      "518/518 [==============================] - 902s 2s/step - loss: 0.6246 - accuracy: 0.6543 - val_loss: 0.5711 - val_accuracy: 0.7098\n",
      "Epoch 3/20\n",
      "518/518 [==============================] - 942s 2s/step - loss: 0.5773 - accuracy: 0.6999 - val_loss: 0.5510 - val_accuracy: 0.7248\n",
      "Epoch 4/20\n",
      "518/518 [==============================] - 922s 2s/step - loss: 0.5352 - accuracy: 0.7337 - val_loss: 0.5216 - val_accuracy: 0.7471\n",
      "Epoch 5/20\n",
      "518/518 [==============================] - 929s 2s/step - loss: 0.5076 - accuracy: 0.7486 - val_loss: 0.5050 - val_accuracy: 0.7590\n",
      "Epoch 6/20\n",
      "518/518 [==============================] - 909s 2s/step - loss: 0.4892 - accuracy: 0.7619 - val_loss: 0.4554 - val_accuracy: 0.7917\n",
      "Epoch 7/20\n",
      "518/518 [==============================] - 870s 2s/step - loss: 0.4698 - accuracy: 0.7753 - val_loss: 0.4333 - val_accuracy: 0.8047\n",
      "Epoch 8/20\n",
      "518/518 [==============================] - 878s 2s/step - loss: 0.4494 - accuracy: 0.7923 - val_loss: 0.4608 - val_accuracy: 0.7902\n",
      "Epoch 9/20\n",
      "518/518 [==============================] - 885s 2s/step - loss: 0.4329 - accuracy: 0.8006 - val_loss: 0.4221 - val_accuracy: 0.8207\n",
      "Epoch 10/20\n",
      "518/518 [==============================] - 882s 2s/step - loss: 0.4096 - accuracy: 0.8124 - val_loss: 0.4057 - val_accuracy: 0.8181\n",
      "Epoch 11/20\n",
      "518/518 [==============================] - 873s 2s/step - loss: 0.4110 - accuracy: 0.8159 - val_loss: 0.5009 - val_accuracy: 0.7585\n",
      "Epoch 12/20\n",
      "518/518 [==============================] - 868s 2s/step - loss: 0.3865 - accuracy: 0.8256 - val_loss: 0.3856 - val_accuracy: 0.8244\n",
      "Epoch 13/20\n",
      "518/518 [==============================] - 882s 2s/step - loss: 0.3811 - accuracy: 0.8276 - val_loss: 0.3750 - val_accuracy: 0.8365\n",
      "Epoch 14/20\n",
      "518/518 [==============================] - 835s 2s/step - loss: 0.3678 - accuracy: 0.8362 - val_loss: 0.3872 - val_accuracy: 0.8326\n",
      "Epoch 15/20\n",
      "518/518 [==============================] - 837s 2s/step - loss: 0.3556 - accuracy: 0.8446 - val_loss: 0.3512 - val_accuracy: 0.8476\n",
      "Epoch 16/20\n",
      "518/518 [==============================] - 864s 2s/step - loss: 0.3472 - accuracy: 0.8463 - val_loss: 0.3544 - val_accuracy: 0.8442\n",
      "Epoch 17/20\n",
      "518/518 [==============================] - 828s 2s/step - loss: 0.3308 - accuracy: 0.8560 - val_loss: 0.3896 - val_accuracy: 0.8256\n",
      "Epoch 18/20\n",
      "518/518 [==============================] - 809s 2s/step - loss: 0.3269 - accuracy: 0.8574 - val_loss: 0.3585 - val_accuracy: 0.8425\n",
      "Epoch 19/20\n",
      "518/518 [==============================] - 839s 2s/step - loss: 0.3190 - accuracy: 0.8616 - val_loss: 0.3221 - val_accuracy: 0.8590\n",
      "Epoch 20/20\n",
      "518/518 [==============================] - 898s 2s/step - loss: 0.3039 - accuracy: 0.8669 - val_loss: 0.3074 - val_accuracy: 0.8728\n",
      "Found 4263 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    ")\n",
    "# The double forward slash // is the floor division operator. It returns the largest integer that is less than or equal to the result of the division.\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "'''\n",
    "The \"shuffle\" parameter in this function is used to shuffle the order of the images in each batch.\n",
    "This is useful for preventing the model from over fitting to the order of the images.\n",
    "When \"shuffle=True\", the order of the images is randomly shuffled at the beginning of each epoch.\n",
    "'''\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "102f91c2",
   "metadata": {},
   "source": [
    "\"model.evaluate()\": Is a method that evaluates the performance of a model on a test dataset.\n",
    "It takes as input a generator that yields batches of test data and returns the scalar loss value and any other metrics specified during the model compilation.\n",
    "In this instance specified parameters are: \"(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\" which we passed to compile method of \"tensorflow.keras.model\".\n",
    "Finally we have two outputs from evaluate method of our model which are sorted exactly from loss to accuracy; Which means the first one is loss and second one is accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fba3756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/134 [==============================] - 31s 227ms/step - loss: 0.2707 - accuracy: 0.8874\n",
      "Test Loss: 0.27069103717803955\n",
      "Test Accuracy: 0.8874032497406006\n"
     ]
    }
   ],
   "source": [
    "# Defining our loss and accuracy variable.\n",
    "loss, accuracy = model.evaluate(test_generator)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ab5683d",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "##### We used the Keras API with TensorFlow to build and train the CNN model. \n",
    "##### The code uses the ImageDataGenerator class to perform data augmentation and preprocessing on the training images.\n",
    "##### The flow_from_directory function is used to load the training, validation, and testing data from the respective directories.\n",
    "##### The CNN model architecture consists of several convolutional and pooling layers, followed by fully connected layers.\n",
    "##### The model is compiled with the Adam optimizer and categorical cross-entropy loss function.\n",
    "##### The model is then trained using the fit function, specifying the training and validation generators.\n",
    "##### Finally, the trained model is evaluated on the testing set using the evaluate function, and the loss and accuracy metrics are printed.\n",
    "##### Note that you'll need to have TensorFlow, Keras and Pillow installed to run these codes.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07c9bda5",
   "metadata": {},
   "source": [
    "### References\n",
    "###### https://keras.io/api/layers/convolution_layers/convolution2d/\n",
    "###### https://keras.io/api/layers/core_layers/dense/ \n",
    "###### https://keras.io/api/layers/reshaping_layers/flatten/ \n",
    "###### https://keras.io/api/layers/pooling_layers/max_pooling2d/\n",
    "###### https://keras.io/guides/sequential_model/\n",
    "###### https://www.tutorialspoint.com/keras/keras_model_compilation.htm#\n",
    "###### https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "###### https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
